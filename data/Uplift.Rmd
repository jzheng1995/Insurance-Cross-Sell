---
title: "Bank Churn Analysis"
author: "Jason"
date: "`r Sys.Date()`"
output: 
  html_document:
    keep_md: true
---
```{r,warning = FALSE, message = FALSE}
library(reticulate)
library(tidyverse)
library(kableExtra)


# tabulate python tables
pytable <- function(object, row = 0){
  if(row == 0){
    py[object] %>% kbl() %>%
  kable_material_dark()
  } else {
    py[object] %>% head(row) %>% kbl() %>%
  kable_material_dark()
  }}
```

# Set up
```{python}
import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
import zipfile 
from zipfile import ZipFile 



# opening the zip file in READ mode 
file_name = "playground-series-s4e7.zip"
with ZipFile(file_name, 'r') as zip: 
    # printing all the contents of the zip file 
    zip.printdir()
    
    
# import
df_zip = zipfile.ZipFile(file_name)
train = pd.read_csv(df_zip.open('train.csv'))
test = pd.read_csv(df_zip.open('test.csv'))
sample_submission = pd.read_csv(df_zip.open('sample_submission.csv')) 
```
# EDA

## Dataset summary

```{python}
train.info()
```

```{python}
len(train)
```

## Sample EDA

Since the dataset is extremely large (11~ million rows), let's do some preliminary EDA on a subset. 
```{python}
import random

n = len(train) # Calculate number of rows in file
s = n//10  # sample size of 10%
skip = sorted(random.sample(range(1, n+1), n-s))   
train_sample = pd.read_csv(df_zip.open('train.csv'), skiprows = skip)
train_sample
```

### Columns, values
```{python}
trainhead = train_sample.head(10)
```
```{r, include = FALSE}
"trainhead" %>% pytable()
```

Based on this output we can further categorize our columns.
- Categorical multi: Region code, Vehicle Age, Policy Sales Channel
- Categorical binary: Gender, Vehicle Age

I suspect Driving License and Vintage are categorical too so let's see what values there are.

```{python}
train_sample["Driving_License"].value_counts()
```


```{python}
train_sample["Vintage"].value_counts()
```
There's enough integers for me to think it represents an actual meaningful interval scale. Since the dataset is based on car insurance, this could be some value attached to insurance for vintage/collector cars. It looks like driving license just represents a binary yes/no so we can leave it for now. 

### Row independence

```{python}
train_sample["id"].value_counts().sort_values( ascending = False).head(10)
```
From our sample of ~1 million rows, we don't see duplicates. While it's good practice to check across the whole dataset, I'm aware this is a fairly curated dataset so it should be safe to say the data points are independent person-wise. 

## Univariate visualization

### y - Response

```{python}
y_counts = train_sample.Response.value_counts()
fig_exit = sns.barplot(x = y_counts.index, y = y_counts.values)
y_counts
train_sample.Response.value_counts(normalize = True)
plt.show()
plt.close("all")
```

In our sample the cross-sell rate is fairly low as an outcome, ~12%. We'll stratify when we do our train-test split. 

## categorical
```{python}
strings = ['object']
string_df = train_sample.select_dtypes(include = strings)
other_cat = train_sample[["Region_Code", "Policy_Sales_Channel"]].astype("category")
cat_df = pd.concat([string_df,other_cat] , axis = 1)

for col in cat_df.columns:
  cat_desc = sns.catplot(x = col, data = cat_df, kind = "count",
                          order = cat_df[col].value_counts().index)
  if col in ["Region_Code", "Policy_Sales_Channel"]:
    # plt.xticks(rotation=90)
    cat_desc.set(xticklabels = [])
  plt.show()
  plt.close("all")
```


## numerical

### univariate 

```{python}
from seaborn import histplot
numeric_df = train_sample.select_dtypes(include = "number")

for col in numeric_df.columns:
  sns.displot(numeric_df, x=col)
  plt.show()
  plt.close("all")
```
Several observations can be made here:
1. Driver license is heavily imbalanced (mostly licensed)
2. Certain regions are more popular
3. Age distribution has two peaks, mid 20s and mid 40s. 
4. Policy sales mostly come from 3 sources
5. Annual premium is very zoomed out. There's likely outlier(s) on the top end

As a first pass-through we'll run the model(s) with these observations baked in. We note them here as possible avenues for model improvement later.

### heat map

```{python}
#correlation map
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(numeric_df.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
# This sets the yticks "upright" with 0, as opposed to sideways with 90.
plt.yticks(rotation=0) 
plt.xticks(rotation=90)
plt.show()
plt.close("all")
```

## x, y split

```{python}
from sklearn.model_selection import train_test_split
from pandas.api.types import CategoricalDtype

# drop id columns
train.drop(["id"],axis = 1, inplace = True)

# features
x_train = train.drop(["Response"], axis = 1)
# label
y_train = train.Response

X_data, X_valid, y_data,y_valid =  train_test_split(x_train, y_train, test_size=0.25,  random_state=42, stratify = y_train)

# sample split
train_sample.drop(["id"],axis = 1, inplace = True)

# features
xs_train = train_sample.drop(["Response"], axis = 1)
# label
ys_train = train_sample.Response

Xs_data, Xs_valid, ys_data,ys_valid =  train_test_split(xs_train, ys_train, test_size=0.25,  random_state=42, stratify = ys_train)
```


### violin 
```{python}
data_dia = ys_train
data_num = xs_train.select_dtypes(include = "number")
data_n_2 = (data_num - data_num.mean()) / (data_num.std())              # standardization
data = pd.concat([ys_train,data_n_2.iloc[:,0:10]],axis=1)
data = pd.melt(data,id_vars="Response",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(10,10))
sns.violinplot(x="features", y="value", hue="Response", data=data,split=True, inner="quart")
plt.xticks(rotation=90)
plt.show()
plt.close("all")
```

```{python}
data_removeoutlier =  data[(data["value"]<3) & (data["value"]>-3)]
data_removeoutlier.info
plt.figure(figsize=(10,10))
sns.violinplot(x="features", y="value", hue="Response", data=data_removeoutlier,split=True, inner="quart")
plt.xticks(rotation=45)
plt.show()
plt.close("all")
```


### heat map

```{python}
#correlation map
f,ax = plt.subplots(figsize=(18, 18))
sns.heatmap(data_num.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
# This sets the yticks "upright" with 0, as opposed to sideways with 90.
plt.yticks(rotation=0) 
plt.xticks(rotation=90)
plt.show()
plt.close("all")
```

# Preprocess pipeline
```{python}
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import FunctionTransformer
from sklearn.impute import SimpleImputer

cat_col = x_train.select_dtypes("object").columns
num_col = x_train.select_dtypes("number").columns

# numerical transformer
num_pipe = Pipeline(steps=[
    ("standardize", StandardScaler())
])
# categorical transformer
cat_pipe = Pipeline(steps=[
    ("ohe", OneHotEncoder(handle_unknown="ignore"))
])

preprocess = ColumnTransformer(
    transformers=[
        ("numerical", num_pipe, num_col),
        ("categorical", cat_pipe, cat_col),
    ],
    remainder = "passthrough"
)
```

# Model fit and evaluation

## Models
```{python}
import xgboost
import lightgbm
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

lor = LogisticRegression()
xgb = XGBClassifier()
rf = RandomForestClassifier()
lgbm = LGBMClassifier()

clfs = {
     'lor':lor,
     'rf':rf,
     'xgb':xgb,
     'lgbm':lgbm
     
}
```

## train function

```{python}
from sklearn.metrics import precision_score,accuracy_score,recall_score,f1_score

def Train_Model(clf,x_train,y_train,x_test,y_test):
     
     pipeline = Pipeline(steps=[
          ('column_tran',preprocess),
          ('model',clf)
     ])

     pipeline.fit(x_train,y_train)
     y_pred = pipeline.predict(x_test)
     acc = accuracy_score(y_test,y_pred)
     ps = precision_score(y_test,y_pred)
     rec = recall_score(y_test,y_pred)
     f1 = f1_score(y_test,y_pred)
     return acc , ps ,rec, f1
```

## Fit

```{python}
acc_score = []
pre_score = []
recall = []
f1s = []

for key,value in clfs.items():
     acc , ps,rs,f1 = Train_Model(value,X_data,y_data,X_valid,y_valid)

     print("name:",key)
     print('ac:',acc)
     print('ps :',ps)
     print('rs:',rs)
     print('f1:',f1)
     
     acc_score.append(acc)
     pre_score.append(ps)
     recall.append(rs)
     f1s.append(f1)
```

## Compare metrics

```{python}
model_df = pd.DataFrame({'Algoritham':clfs.keys(),'Accuracy Score':acc_score,'Precision Score':pre_score,'Recall Score':recall,'F1 Score':f1s}).sort_values('Accuracy Score',ascending=False)
```
```{r}
"model_df" %>% pytable()
```

## Voting classifier

```{python}
from sklearn.ensemble import VotingClassifier

vote = VotingClassifier(estimators=[
     ('xgb',xgb),
     ('random forest',rf),
     ('lgbm',lgbm)], voting = 'soft')

vote_pipeline = Pipeline(steps=[
     ('colums_tran',preprocess),
     ('model',vote)
])

vote_pipeline.fit(x_train,y_train)
```

Save model object
```{r, eval = FALSE}
import joblib
joblib.dump(vote_pipeline, 'vote_classifier', compress=9)
```
# Predict - submission

Check labels
```{python}
# labels
vote_pipeline.classes_
```

Create and save submission dataframe
```{python}
# prediction array
y_test = vote_pipeline.predict_proba(test)

# submission df
my_submission = pd.DataFrame({'id': test["id"] , 'Exited': y_test[:,1]})

# save
my_submission.to_csv('submission.csv', index=False)
```




